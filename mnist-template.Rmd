---
title: "Modeling MNIST Data with Keras"
output:
  html_document:
    df_print: paged
---

# Introduction

The MNIST database is a set of images containing handwritten digits from 0 to 9.

Each image has dimensions of 28x28, and the dataset assigns a value to each pixel representing the color of that pixel. The dataset also contains a label for each image which tells us which digit the image represents.

In this exercise, we will walk through the steps to create a deep learning model which predicts the digit that is represented by a particular image.

# Prerequisites

Before beginning to create our model, we need to make sure that we have the necessary software. To utilize the `keras` package in R, we need the following:

* [R](https://cran.r-project.org/)
* [RStudio](https://www.rstudio.com/)
* A Python distribution, such as [Anaconda](https://www.anaconda.com/)

Once we have the software installed, we can install the `keras` package in RStudio using `install.packages("keras")`. Next, we install Keras and the `TensorFlow` backend using the `install_keras()` function. Now we are ready to begin!

# Loading the Package and Dataset

First we load the `keras` package:

```{r, warning=FALSE}
library(keras)
```

Next we load the MNIST dataset:

```{r}
mnist <- dataset_mnist()
str(mnist)
```

As seen in the output above, the MNIST dataset has a training set of 60,000 images and a test set of 10,000 images. Within each set, the `x` arrays contain 28x28 matrices of the image pixels, while the `y` arrays contain the digit represented by the image.

We define each of these components as its own object:

```{r}
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y
```

# Manipulating the Data

Before we can create our model, we need to format our data so that it can be read in by Keras. First we transform the `x` arrays, which are three-dimensional, into a two-dimensional matrix by converting the 28x28 matrix to a vector with 784 values. We do this using Keras's `array_reshape()` function:

```{r}
x_train <- array_reshape(x_train, c(nrow(x_train), 784))
x_test <- array_reshape(x_test, c(nrow(x_test), 784))
str(x_train)
```

Next, we will rescale the `x` values so that they are between 0 and 1 instead of between 0 and 255:

```{r}
x_train <- x_train / 255
x_test <- x_test / 255
```

Lastly, we transform the `y` vectors into a categorical matrix, where for each row the column with a value of one is the column representing the desired `y` value. We use Keras's `to_categorical()` function:

```{r}
y_train <- to_categorical(y_train, 10)
y_test <- to_categorical(y_test, 10)
head(y_train)
```

# Creating the Model

We will create a sequential model for our dataset. A sequential model is just a linear stack of layers. The model generated by the code below consists of 5 layers:

```{r}

```

Going through the code, we see that our model has 3 dense layers and 2 dropout layers. The dense layer is our typical neural network layer, with input being mapped to output. We tell our first layer that our input is a vector with 784 values (generated from our images above). We first map our inputs to 256 outputs, then 128 outputs, and finally the 10 outputs which correspond to the digits 0 to 9. The activation function defines how the layer relates the input and output.

The dropout layers randomly assign a certain portion of the weights to be 0. This helps to prevent overfitting.

We can see the details of our model below:

```{r}

```

We see that we have 235,146 parameters which we can train.

Our next step is to compile our model:

```{r}

```

The code above defines the loss function we want to use, the optimizer, and which metrics we want to use to test the model.

# Training and Evaluating the Model

The next step is to train our model using our data:

```{r}

```

We are training the model over 30 epochs (iterations) with a batch size of 128 images, while using 20% of our data for the validation set.

We can see the performance of our model in the plot below:

```{r}

```

As we ran through additional epochs of model training, the loss function decreased and the accuracy increased.

We can evaluate our model's performance using the test data:

```{r}

```

Lastly, we can view some sample input images and have our model predict the digits they represent:

```{r}
par(mfrow=c(3,5))
x_test_plot <- mnist$test$x / 255
for(i in 1:15) {
  plot(as.raster(x_test_plot[i,,]))
}
```

```{r}

```

Looks like our model did pretty well!